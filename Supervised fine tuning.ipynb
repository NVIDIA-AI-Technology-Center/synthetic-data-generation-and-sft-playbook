{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ac2b61-ccdf-41f6-872e-d7ab3705ae8b",
   "metadata": {},
   "source": [
    "# Supervised Fine Tuning of Llama 3.1 through using Synthetic Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceab580-e8f6-4d3c-b56a-6275a087b732",
   "metadata": {},
   "source": [
    "This playbook demonstrates how to fine tune a Llama 3.1 Instruct model on synthetically enriched data through using the NeMo Framework. Synthetic data is generated by querying Nemotron-4 340B Instruct via NVIDIA Services.\n",
    "\n",
    "> NOTE: Ensure that you run this notebook inside the [NeMo Framework container](https://github.com/NVIDIA/NeMo) (24.07) which has all the required dependencies. Instructions are available in the associated tutorial README to download the model and the container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04a075-727c-4b4d-9300-65fea267849b",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions to Download and Launch an NGC Container\n",
    "\n",
    "**Step 1: Set Up Your NGC Account and API Key**\n",
    "- Create an NGC Account: Go to https://ngc.nvidia.com/signup and create an account.\n",
    "- Generate an NGC API Key: Log in to your account, navigate to your account settings, and generate an API key.\n",
    "\n",
    "**Step 2: Log In to the NGC Container Registry**\n",
    "- To access NGC containers, you need to log in to the NVIDIA container registry using your API key.\n",
    "> ```docker login nvcr.io```\n",
    "- When prompted for the username, enter: ```$oauthtoken```\n",
    "- For the password, enter your NGC API key.\n",
    "\n",
    "**Step 3: Pull the Desired Container from NGC**\n",
    "\n",
    "- Replace <container-name> and <tag> with the specific container and version you want to use.\n",
    "\n",
    "> ```docker pull docker://nvcr.io/nvidia/nemo:24.07```\n",
    "\n",
    "**Step 4: Launch the Container**\n",
    "\n",
    "> ```docker run --gpus all --rm -it nvcr.io/nvidia/nemo:24.07```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5a8d-38b4-4b80-9fd3-cf6afb05adf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "- Access to NGC Services for the generation of synthetic data \n",
    "- At least 2 GPUs (A100 80 GB) for model fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc9774-74b8-4a6c-bbb2-4d019f976bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Section 1: Synthetic Data Generation\n",
    "\n",
    "This section demonstrates the use of a basic script that loops on two stages as follows. The number of stages can be increased until the desired accuracy or dataset size is achieved:\n",
    "\n",
    "\n",
    "1. **Data processing**: perform operations such as HTML tag cleaning, quality-based filtering and semantic deduplication on the records. \n",
    "1. **Synthetic data generation**: query a synthetic data generation model (such as LLaMa 3.1 405B Instruct, or Nemotron-4 340B Instruct) to produce synthetic variants of existing records. Each synthetic record is then fed to a reward model (such as Nemotron-4 340B Reward), and assigned a quality score. All records are then fed to the data processing stage for further processing. In this specific example the NVIDIA API Nemotron-4 340B endopoint is used to generated synthetic data into the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd9031-2a2d-4541-9b26-0aba14e2dada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from IPython.display import FileLink, FileLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b60bf-94fd-43c5-b28a-bc2131101386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEMO_DIR = os.path.join(\"/opt/NeMo\")\n",
    "NEMO_CURATOR_DIR = os.path.join(\"/opt/NeMo-Curator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b1189-e9b4-4fe3-aa1a-c61d51e3f92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_WORKING_DIR = os.path.join(os.path.expanduser('~'), \"exp1\") #FIXME: Define your working dir \n",
    "os.makedirs(YOUR_WORKING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3448b5-2dae-49f3-bb34-82369ca94d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can get free API credits registering at: https://build.nvidia.com\n",
    "NVIDIA_SERVICES_TOKEN=\"\" #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc1a4b-fcec-4d6c-9a56-0f72e491bd03",
   "metadata": {},
   "source": [
    "Check whether your working dir was successfully created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a2451-943d-41d0-b4f0-8417ecf733ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -ld {NEMO_DIR} {NEMO_CURATOR_DIR} {YOUR_WORKING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338747a9-09c1-4615-b4dd-938bb931f215",
   "metadata": {},
   "source": [
    "Double check your current path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece08599-879a-4e4a-924b-9a0750b34e55",
   "metadata": {},
   "source": [
    "Launch the generation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056733c-a81d-4801-a1da-b27f493a6482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python peft-curation-with-sdg/main.py \\\n",
    "    --api-key {NVIDIA_SERVICES_TOKEN} \\\n",
    "    --device gpu \\\n",
    "    --synth-gen-rounds 1 --synth-gen-ratio 0.001 --synth-gen-model \"nvidia/nemotron-4-340b-instruct\" \\\n",
    "    --working-dir {YOUR_WORKING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34dec3-44ce-4cfc-bb66-009a7bdb13f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a03f1e-efa7-4ca2-bbb3-73f2c9fa0467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l {YOUR_WORKING_DIR}/data/curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71af2d-296b-4512-8e3c-c4d075478166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(YOUR_WORKING_DIR, \"data/curated/final\")\n",
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535932fd-5169-4731-8d6a-44e16363122f",
   "metadata": {},
   "source": [
    "You should see the law-qa-{train/val/test}.jsonl splits resulting from following the abovementioned SDG tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779e0bc-9397-4283-b270-8f7ad23d07a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DS = os.path.join(DATA_DIR, \"law-qa-train.jsonl\")\n",
    "VAL_DS = os.path.join(DATA_DIR, \"law-qa-val.jsonl\")\n",
    "TEST_DS = os.path.join(DATA_DIR, \"law-qa-test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5cdd4-4ceb-4b6c-aa33-0d91afa01cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "##  Section 2: Supervised Fine Tuning\n",
    "\n",
    "This section is structured as follows:\n",
    "1. Download the model to finetune \n",
    "1. Prepare the dataset\n",
    "1. Run the PEFT finetuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475af12d-7e42-44a0-9902-cf5057b1abbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Download the `Meta Llama 3.1 8B Instruct .nemo` model and mount the corresponding folder to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8e34-784e-4012-8cd6-63022bd16a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {YOUR_WORKING_DIR}/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4ad89-9feb-4b62-86ae-8c760c73897e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/nemo/llama-3_1-8b-instruct-nemo/versions/1.0/zip -O {YOUR_WORKING_DIR}/model/llama-3_1-8b-instruct-nemo_1.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d844e3-1302-4e11-93af-455cf0fa9811",
   "metadata": {},
   "source": [
    "Uncompress the model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a6db7-225e-44e7-8b84-ee09b5c901ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip {YOUR_WORKING_DIR}/model/llama-3_1-8b-instruct-nemo_1.0.zip -d {YOUR_WORKING_DIR}/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6fd26-0893-47b2-a2ac-f9163f62b85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR=os.path.join(YOUR_WORKING_DIR, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3e804-c876-469c-af80-7018ec731555",
   "metadata": {},
   "source": [
    "You should have now two distinct files. The archive (.zip) and the uncompressed model (.nemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dad1db-3842-421a-ad9c-8704302e7764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535885f-1932-438e-94db-ab63b8cc3934",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the dataset\n",
    "\n",
    "The dataset has already undergone several filtering and processing operations, and it can be used to finetune the model for various different tasks - summarization, multi-label classification.\n",
    "\n",
    "Let's take a look at a single row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283230b-a4b5-42b4-a8ac-04b954828152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN, VAL and TEST splits all follow the same structure\n",
    "!head -n1 {TRAIN_DS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44568f3e-7c32-4061-9f75-0553a1f0f151",
   "metadata": {},
   "source": [
    "You will see several fields in the `.jsonl`, including `title`, `question`, `answer`, and other associated metadata.\n",
    "\n",
    "For this tutorial, our input will be the `answer` field, and output will be it's `title`. \n",
    "\n",
    "The following cell does two things -\n",
    "* Adds a template - a prompt instruction (which is optional), and format `{PROMPT} \\nQUESTION: {data[\"question\"]} \\nTITLE: `.\n",
    "* Saves the data splits into the same location, also appending a `_preprocessed` marker to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d94739-8a20-4f58-af23-15e79e729a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a prompt instruction.\n",
    "PROMPT='''Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more.'''\n",
    "\n",
    "# Creates a preprocessed version of the data files\n",
    "for input_file in [TRAIN_DS, VAL_DS, TEST_DS]:\n",
    "    output_file = input_file.rsplit('.', 1)[0] + '_preprocessed.jsonl'\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Parse each line as JSON\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Create a new dictionary with only the desired fields, renamed and formatted\n",
    "            new_data = {\n",
    "                \"input\": f'''{PROMPT} \\nQUESTION: {data[\"question\"]} \\nTITLE: ''',\n",
    "                \"output\": data['title']\n",
    "            }\n",
    "\n",
    "            # Write the new data as a JSON line to the output file\n",
    "            json.dump(new_data, outfile)\n",
    "            outfile.write('\\n')  # Add a newline after each JSON object\n",
    "\n",
    "    print(f\"Processed {input_file} and created {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e1ff2-ed53-4a86-a95b-f81775168363",
   "metadata": {},
   "source": [
    "After running the above scripts, you will see  `law-qa-{train/test/val}_preprocessed.jsonl` files appear in the data directory.\n",
    "\n",
    "This is what an example will be formatted like -\n",
    "\n",
    "```json\n",
    "{\"input\": \"Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more. \\nQUESTION: In order to be sued in a particular jurisdiction, say New York, a company must have a minimal business presence in the jurisdiction. What constitutes such a presence? Suppose the company engaged a New York-based Plaintiff, and its representatives signed the contract with the Plaintiff in New York City. Does this satisfy the minimum presence rule? Suppose, instead, the plaintiff and contract signing were in New Jersey, but the company hired a law firm with offices in New York City. Does this qualify? \\nTITLE: \", \n",
    " \"output\": \"What constitutes \\\"doing business in a jurisdiction?\\\"\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f9ae9-a6f7-489d-ba54-b3d605692988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear up any cached mem-map file\n",
    "!rm {DATA_DIR}/*idx*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07088893-d186-4b9e-b713-bb7b0d4045ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='sft'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05210705-303a-4b55-9b9e-37befa86f7da",
   "metadata": {},
   "source": [
    "### Step 3: Run PEFT finetuning script for LoRA\n",
    "\n",
    "NeMo framework includes a high level python script for fine-tuning  [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train, test and validation data files as well as path to the .nemo model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b6137-a3f6-41a2-8b1a-a951bd3eafe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following script runs a finetuning job on 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cb8ea-d606-4faa-880c-762c47cc20a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYTHONWARNINGS=\"ignore\"\n",
    "OMP_NUM_THREADS=16\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "MODEL=\"/root/exp1/model/llama3_1_8b_instruct.nemo\" #FIXME\n",
    "\n",
    "TRAIN_DS=\"[/root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl]\" #FIXME\n",
    "VALID_DS=\"[/root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl]\"   #FIXME\n",
    "TEST_DS=\"[/root/exp1/data/curated/final/law-qa-test_preprocessed.jsonl]\"   #FIXME\n",
    "TEST_NAMES=\"[law]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "OUTPUT_DIR=\"/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen\"\n",
    "rm -r $OUTPUT_DIR\n",
    "\n",
    "torchrun --nproc_per_node=4 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=4 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.2 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=32 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b37c2-26bb-4f07-b635-1cdfb4d9136f",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `{YOUR_WORKING_DIR}/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above -\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set \n",
    "\n",
    "```bash\n",
    "model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "```\n",
    "You can override many such configurations (such as `learning rate`, `adapter dim`, and more) while running the script. A full set of possible configurations is available in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7db13a-2d7b-4d21-87ee-d7bae7bc97b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Section 3: Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11c787-fe71-438a-a97a-8007261d03bc",
   "metadata": {},
   "source": [
    "### Step 1: Run the finetuned model and generate output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709a1d3-333c-4fd2-a1d1-e4f24596c4a0",
   "metadata": {},
   "source": [
    "Now we can execute the model and the check the generated output. Note this is more for testing and validation, not a full-fledged  deployment solution like NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db1cc0-4ee7-4085-bf54-47535c65c984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that the LORA model file exists\n",
    "!ls -l {YOUR_WORKING_DIR}/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c7656-177d-4af6-ad14-8b0bc769264b",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting - \n",
    "\n",
    "1. `model.restore_from_path` to the path for the Meta-Llama-3.1-8B-Instruct.nemo file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the preprocessed test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081844f-f173-41a0-ac39-cd84755a9e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a smaller test subset for a quick eval demonstration.\n",
    "\n",
    "!head -n 128 {DATA_DIR}/law-qa-test_preprocessed.jsonl > {DATA_DIR}/law-qa-test_preprocessed-n128.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a37ec5-8fc7-4499-8248-1d825badbf95",
   "metadata": {},
   "source": [
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2825989-910a-4a40-a13f-fea58b6cf07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL=\"/root/exp1/model/llama3_1_8b_instruct.nemo\"\n",
    "\n",
    "TEST_DS=\"[/root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl]\" # Smaller test split\n",
    "# TEST_DS=\"[./curated-data/law-qa-test_preprocessed.jsonl]\" # Full test set\n",
    "TEST_NAMES=\"[law]\"\n",
    "\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"law_titlegen_lora\"\n",
    "\n",
    "TOKENIZERS_PARALLELISM=\"true\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=32 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=25 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True  \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.truncation_field=\"null\" \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\}\\ \\{output\\}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568370-0486-44b4-8fb9-64cb266a2500",
   "metadata": {},
   "source": [
    "### Step 2: Check the model accuracy\n",
    "\n",
    "Now that the results are in, let's read the results and calculate the accuracy on the question title generation task.\n",
    "Let's take a look at one of the predictions in the generated output file. The `pred` key indicates what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2fc890-5b66-4986-b084-6910ff420003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look at predictions\n",
    "!head -n1  law_titlegen_lora_test_law_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d6079-315e-490e-9b74-e41cb9d96cbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "For evaluating this task, we will use [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)).  It measures overlap of ngrams, and a higher score is better. While it's not perfect and it misses capturing the semantics of the prediction, it is a popular metric in academia and industry for evaluating such systems. \n",
    "\n",
    "The following method uses the `rouge_score` library to implement scoring. It will report `ROUGE_{1/2/L/Lsum}` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd71ca0-5154-4a6b-adbf-a8c40a565d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_rouge(input_file: str) -> dict:\n",
    "    ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(ROUGE_KEYS, use_stemmer=True)\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    lines = [json.loads(line) for line in open(input_file)]\n",
    "    num_response_words = []\n",
    "    num_ref_words = []\n",
    "    for idx, line in enumerate(lines):\n",
    "        prompt = line['input']\n",
    "        response = line['pred']\n",
    "        answer = line['label']\n",
    "        scores = scorer.score(response, answer)\n",
    "        aggregator.add_scores(scores)\n",
    "        num_response_words.append(len(response.split()))\n",
    "        num_ref_words.append(len(answer.split()))\n",
    "\n",
    "    result = aggregator.aggregate()\n",
    "    rouge_scores = {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
    "    print(rouge_scores)\n",
    "    print(f\"Average and stddev of response length: {np.mean(num_response_words):.2f}, {np.std(num_response_words):.2f}\")\n",
    "    print(f\"Average and stddev of ref length: {np.mean(num_ref_words):.2f}, {np.std(num_ref_words):.2f}\")\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be1472-1a63-41c1-86b2-fd4404cf9928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_rouge(\"./law_titlegen_lora_test_law_inputs_preds_labels.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fd720-d9bd-4ccb-8582-2b62e4e5e31e",
   "metadata": {},
   "source": [
    "For the Llama-3.1-8B-Instruct model, you should see accuracy comparable to the below:\n",
    "```\n",
    "{'rouge1': 39.2082, 'rouge2': 18.8573, 'rougeL': 35.4098, 'rougeLsum': 35.3906}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
